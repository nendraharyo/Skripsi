{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dask.array as da\n",
    "import dask_ml.preprocessing as dm\n",
    "import dask.dataframe as dd\n",
    "import random\n",
    "import vaex as vx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solusi 1 pake pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randompick(filename):\n",
    "    n = sum(1 for line in open(filename)) - 1 #number of records in file (excludes header)\n",
    "    s = 3000000 #desired sample size\n",
    "    skip = sorted(random.sample(range(1,n+1),n-s)) #the 0-indexed header will not be included in the skip list\n",
    "    df = pd.read_csv(filename, skiprows=skip)\n",
    "    return df\n",
    "\n",
    "dfb=randompick(r\"D:\\DatasetSkripsi\\Cleaned\\balanced-benign.csv\")\n",
    "dfd=randompick(r\"D:\\DatasetSkripsi\\Cleaned\\balanced-ddos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfb=dfb.replace('Benign',0)\n",
    "dfd=dfd.replace('ddos',1)\n",
    "le = dm.LabelEncoder()\n",
    "def labelencode(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == type(object):\n",
    "            print(column)\n",
    "            df[column] =le.fit_transform(df[column].astype(str))\n",
    "    return df\n",
    "dfb=labelencode(dfb)\n",
    "dfd=labelencode(dfd)\n",
    "df=pd.concat([dfb,dfd])\n",
    "dfb=0\n",
    "dfd=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    df.fillna(0,inplace=True)\n",
    "    df[df<0]=0\n",
    "    return df\n",
    "df=clean_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_dfbalanced=df.iloc[:,1:84]\n",
    "Y_dfbalanced=df.iloc[:,84]\n",
    "df=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Suppose, we select 5 features with top 5 Fisher scores\n",
    "selector = SelectKBest(f_classif, k = 10)\n",
    "#New dataframe with the selected features for later use in the classifier. fit() method works too, if you want only the feature names and their corresponding scores\n",
    "selector.fit(X_dfbalanced, Y_dfbalanced)\n",
    "cols = selector.get_support(indices=True)\n",
    "features_df_new = X_dfbalanced.iloc[:,cols]\n",
    "names = X_dfbalanced.columns.values[selector.get_support()]\n",
    "scores = selector.scores_[selector.get_support()]\n",
    "names_scores = list(zip(names, scores))\n",
    "ns_df = pd.DataFrame(data = names_scores, columns=['Feat_names', 'F_Scores'])\n",
    "#Sort the dataframe for better visualization\n",
    "ns_df_sorted = ns_df.sort_values(['F_Scores', 'Feat_names'], ascending = [False, True])\n",
    "print(ns_df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_dfbalanced=0\n",
    "X=features_df_new\n",
    "Y=Y_dfbalanced\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'max_depth': 3,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 10}  # the number of classes that exist in this datset\n",
    "num_round = 20\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "print(best_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(precision_score(y_test, best_preds, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import cv\n",
    "xgb_cv = cv(dtrain=dtrain, params=params, nfold=5,\n",
    "                    num_boost_round=20, early_stopping_rounds=10, metrics=\"error\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solusi 2 pake Dask tapi gabisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=vx.read_csv(r\"D:\\DatasetSkripsi\\ddos_balanced\\final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntac of where: (condition, fill value, column to fill)\n",
    "df['Label'] = df.func.where(df.Label == 'Benign', 0, df.Label)\n",
    "df['Label'] = df.func.where(df.Label == 'ddos', 1, df.Label)\n",
    "le = dm.LabelEncoder()\n",
    "def labelencode(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == type(object):\n",
    "            print(column)\n",
    "            df[column] =le.fit_transform(df[column].astype(str))\n",
    "    return df\n",
    "df=labelencode(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}